# Mann Whitney U test for loan amount
amount_test <- wilcox.test(observed$loan_amnt~observed$loan_status)
# output results
statistically_significant(amount_test$p.value)
# Mann Whitney U test for interest
interest_test <- wilcox.test(observed$loan_int_rate~observed$loan_status)
# output results
statistically_significant(interest_test$p.value)
# Mann Whitney U test for percent income
percent_income_test <- wilcox.test(observed$loan_percent_income~observed$loan_status)
# output results
statistically_significant(percent_income_test$p.value)
# Mann Whitney U test for credit history
credit_history_test <- wilcox.test(
observed$cb_person_cred_hist_length~observed$loan_status)
# output results
statistically_significant(credit_history_test$p.value)
# remove columns that are not statistically significant predictors
prior <- prior %>%
subset(select=-c(person_age,person_emp_length,cb_person_cred_hist_length))
observed <- observed %>%
subset(select=-c(person_age,person_emp_length,cb_person_cred_hist_length))
test <- test %>%
subset(select=-c(person_age,person_emp_length,cb_person_cred_hist_length))
# outout levels in target
kable(table(prior$loan_status), col.names=c('Level','Count')) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
# outout levels in target
kable(table(observed$loan_status), col.names=c('Level','Count')) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
# x values for functions
x <- seq(0.21,0.32,length=1000)
# function for plotting bayes model
bayes_model_plot <- function(model){
ggplot(model) +
geom_line(aes(x=x,y=Density,color=Model)) +
labs(title='Bayesian Model') + ylab('Density') +
scale_color_manual(name='Models',
values=c('Prior'='deepskyblue2',
'Observed'='darkorange2',
'Posterior'='maroon3'),
labels=c('Prior','Observed','Posterior')) +
theme(plot.title=element_text(face='bold'))
}
# obtain parameters
n <- nrow(observed)
y <- sum(observed$loan_status)
# create parameters for sampling distribution
observed_dist <- dbeta(x,y+1,n-y+1)
# obtain parameters
alpha <- sum(prior$loan_status)
beta <- nrow(prior)-alpha
a_post <- alpha+y
b_post <- beta+n-y
# create the prior and posterior distributions
prior_dist <- dbeta(x,alpha,beta)
posterior_dist <- dbeta(x,a_post,b_post)
# create model name vectors
prior_name <- rep('Prior',length(x))
observed_name <- rep('Observed',length(x))
posterior_name <- rep('Posterior',length(x))
# create data frames
prior_df <- data.frame(prior_name,prior_dist,x)
prior_df <- prior_df %>% plyr::rename(c('prior_name'='Model','prior_dist'='Density'))
observed_df <- data.frame(observed_name,observed_dist,x)
observed_df <- observed_df %>%
plyr::rename(c('observed_name'='Model','observed_dist'='Density','x'))
posterior_df <- data.frame(posterior_name,posterior_dist,x)
posterior_df <- posterior_df %>%
plyr::rename(c('posterior_name'='Model','posterior_dist'='Density'))
conjugate <- rbind(prior_df,observed_df,posterior_df)
# create graph
bayes_model_plot(conjugate)
# monte carlo simulation function
monte_carlo <- function(size){
T_ytilde <- c()
for (s in 1:1000){
# obtain theta_s from posterior distribution
theta_s <- rbeta(1,a_post,b_post)
# obtain 10 y values from sampling model with parameter theta_s
y_s <- rbinom(10,size,theta_s)
# calculate the test statistic
t_s <- mean(y_s)
T_ytilde <- c(T_ytilde,t_s)
}
return(T_ytilde)
}
# monte carlo simulation graphs
monte_carlo_plot <- function(test_stat, real){
ggplot() +
geom_histogram(aes(x=test_stat,y=..density..),bins=20,alpha=0.5,fill='deepskyblue2') +
geom_density(aes(x=test_stat),alpha=0.3,color='deepskyblue2',fill='deepskyblue2') +
geom_segment(
aes(x=real,y=0,xend=real,yend=Inf),color='darkorange2',linetype='dashed') +
labs(title='Monte Carlo Simulation') +
xlab('Number of Defaults') + ylab('Density') +
theme(plot.title=element_text(face='bold'))
}
# obtain test statistic for observed and simulated data
T_yobs <- sum(observed$loan_status)
T_ytilde <- monte_carlo(n)
# create plot
monte_carlo_plot(T_ytilde,T_yobs)
# obtain test statistic for observed and simulated data
T_ytest <- sum(test$loan_status)
T_ytilde <- monte_carlo(nrow(test))
# create plot
monte_carlo_plot(T_ytilde,T_ytest)
# function for plotting the credible interval
cred_int_plot <- function(model,lower,upper){
ggplot() +
geom_line(aes(x,model),color='deepskyblue2') +
geom_area(mapping=aes(x=ifelse(x>lower & x<upper,x,0),y=model),
fill='deepskyblue2', alpha=0.3) +
xlim(0.23,0.31) +
labs(title='Credible Interval') +
xlab('Theta') + ylab('Density') +
theme(plot.title=element_text(face='bold'))
}
# MLE for observed data
p <- sum(observed$loan_status)/nrow(observed)
# obtain bounds
bounds <- qbeta(c(0.025,0.975),a_post,b_post)
# print bounds
cat('Lower bound: ',bounds[1],'\n')
cat('Upper bound: ',bounds[2],'\n')
# plot credible interval with MLE from observed data
cred_int_plot(posterior_dist,bounds[1],bounds[2]) +
geom_segment(aes(x=p,y=0,xend=p,yend=Inf),color='darkorange2',linetype='dashed')
# observed data
h_model_obs <- observed %>%
group_by(person_home_ownership) %>%
summarise(y=sum(loan_status),n=n())
# test data
h_test <- test %>%
group_by(person_home_ownership) %>%
summarise(y=sum(loan_status),n=n()) %>%
mutate(percent=y/n)
# add test data to h_model
h_model_obs$test <- h_test$percent
# obtain values
y <- h_model_obs$y
n <- h_model_obs$n
J <- nrow(h_model_obs)
# no pooling estimate
no_pool <- y/n
# output the no pooling estimate for each group
for (i in 1:length(h_model_obs$person_home_ownership)){
cat('No pooling estimate for ',
as.vector(h_model_obs$person_home_ownership)[i],' : ',no_pool[i],'\n')
}
# complete pooling estimate
complete_pool <- sum(y)/sum(n)
# output the complete pooling estimate
cat('Complete pooling estimate: ',complete_pool)
# function for counting number of positive in a bootstrap sample
bootstrap <- function(vector,num_samp){
num_pos <- c()
for (i in 1:num_samp){
set.seed(187+i)
samp <- sample(vector,nrow(prior),replace=TRUE)
num_pos <- c(num_pos,sum(samp))
}
return(num_pos)
}
# function for plotting bootstrapped sample
bootstrap_plot <- function(samples, mu){
ggplot() +
geom_histogram(aes(x=samples,y=..density..),bins=20,alpha=0.5,fill='deepskyblue2') +
geom_density(aes(x=samples),alpha=0.3,color='deepskyblue2',fill='deepskyblue2') +
geom_segment(aes(x=mu,y=0,xend=mu,yend=Inf),color='darkorange2',linetype='dashed') +
labs(title='Bootstrap Sample') +
xlab('Number of Observations') + ylab('Density') +
theme(plot.title=element_text(face='bold'))
}
# obtain samples of sample size
pos_samples <- bootstrap(prior$loan_status,1000)
p <- sum(prior$loan_status)/nrow(prior)
samples <- pos_samples/p
# output mean and standard deviation
cat("Mean of samples: ",mean(samples),'\n')
cat("Standard deviation of samples: ",sd(samples),'\n')
# plot bootstrap samples
bootstrap_plot(samples,mean(samples))
# apriori parameters
m <- mean(samples)
std <- sd(samples)
# attach stan file
h_model <- stan_model('~/Documents/MachineLearning/Bayes/loan_status.stan')
# fit posterior distribution
set.seed(987)
stan_fit <- rstan::sampling(
h_model, data=list(J=J,y=y,n=n,a=alpha,b=beta,m=m,std=std),refresh=0)
# samples from posterior distribution
h_model_samples <- rstan::extract(stan_fit)
# obtain values
shrinkage <- colMeans(h_model_samples$theta)
# output shrinkage estimate for each group
for (i in 1:length(h_model_obs$person_home_ownership)){
cat('Prior data shrinkage estimate for ',
as.vector(h_model_obs$person_home_ownership)[i],
' : ',shrinkage[i],'\n')
}
# obtain data frame
thetas_df <- data.frame(h_model_samples$theta)
thetas_df <- reshape2::melt(thetas_df)
# create histogram
ggplot(thetas_df,aes(x=value,fill=variable)) +
geom_histogram(aes(y=..density..),bins=20,alpha=0.5,position='identity') +
geom_density(aes(color=variable),alpha=0.3) +
scale_color_manual(name='Theta',
values=c('X1'='deepskyblue2',
'X2'='darkorange2',
'X3'='maroon3'),
labels=c('Mortgage','Own','Rent')) +
scale_fill_manual(name='Theta',
values=c('X1'='deepskyblue2',
'X2'='darkorange2',
'X3'='maroon3'),
labels=c('Mortgage','Own','Rent')) +
labs(title='Histogram of Hierarchical Thetas') +
xlab('Theta') + ylab('Density') +
theme(plot.title=element_text(face='bold'))
# shrinkage plot
ggplot() +
geom_segment(aes(x=no_pool,xend=shrinkage,y=1,yend=0,
color='Shrinkage Estimates')) +
geom_point(aes(x=no_pool,y=1),color='deepskyblue2') +
geom_point(aes(x=shrinkage,y=0),color='deepskyblue2') +
geom_segment(aes(x=mean(shrinkage),xend=mean(shrinkage),y=0,yend=1,
color='Shrinkage Mean')) +
geom_point(aes(x=mean(shrinkage),y=1),color='darkorange2') +
geom_point(aes(x=mean(shrinkage),y=0),color='darkorange2') +
geom_segment(aes(x=complete_pool,xend=complete_pool,y=0,yend=1,
color='Complete Pooling Mean')) +
geom_point(aes(x=complete_pool,y=1),color='maroon3') +
geom_point(aes(x=complete_pool,y=0),color='maroon3') +
labs(title='Shrinkage Plot') +
xlab('Theta') + ylab('') +
scale_y_continuous(breaks=c(0, 1),labels=c("Bayes", "MLE"),limits=c(0,1)) +
scale_color_manual(name='Legend',values=c('Shrinkage Estimates'='deepskyblue2',
'Shrinkage Mean'='darkorange2',
'Complete Pooling Mean'='maroon3')) +
theme(plot.title=element_text(face='bold'))
# rmse function
rmse <- function(y,y_hat){
return(sqrt(mean((y-y_hat)^2)))
}
# output results
cat('No pooling RMSE: ',rmse(h_model_obs$test,no_pool),'\n')
cat('Complete pooling RMSE: ',rmse(h_model_obs$test,complete_pool),'\n')
cat('Shrinkage RMSE',rmse(h_model_obs$test,colMeans(h_model_samples$theta)),'\n')
# create train
train <- observed
# one hot encode person_home_ownership
train$mortgage <- ifelse(as.character(train$person_home_ownership)=='MORTGAGE',1,0)
train$own <- ifelse(as.character(train$person_home_ownership)=='RENT',1,0)
train <- train %>% subset(select=-c(person_home_ownership))
# create predictor and response
train_X <- train %>% subset(select=-c(loan_status))
train_y <- train$loan_status
# scale train_X
norm_pars <- preProcess(train_X)
train_X <- predict(norm_pars,train_X)
# create test
test_logistic <- test
# one hot encode person_home_ownership
test_logistic$mortgage <-
ifelse(as.character(test_logistic$person_home_ownership)=='MORTGAGE',1,0)
test_logistic$own <-
ifelse(as.character(test_logistic$person_home_ownership)=='RENT',1,0)
test_logistic <- test_logistic %>% subset(select=-c(person_home_ownership))
# create predictor and response
test_logistic_X <- test_logistic %>% subset(select=-c(loan_status))
test_logistic_y <- test_logistic$loan_status
# scale train_X
test_logistic_X <- predict(norm_pars,test_logistic_X)
# function for making predictions
logistic_predictions <- function(X,betas){
X_new <- X
X_new$ones <- rep(1,nrow(X_new))
sums <- colSums(t(X_new)*betas)
predictions <- c()
for (i in 1:length(sums)){
predictions <- c(predictions, exp(sums[i])/(1+exp(sums[i])))
}
return(predictions)
}
# function for plotting predictions
pred_plot <- function(pred,observe){
ggplot() +
geom_histogram(aes(x=prob,fill=observe,y=..density..),
bins=20,alpha=0.5,position='identity') +
geom_density(aes(x=prob,fill=observe,color=observe),alpha=0.3) +
scale_color_manual(name='Observed',
values=c('0'='deepskyblue2',
'1'='darkorange2'),
labels=c('No Default','Default')) +
scale_fill_manual(name='Observed',
values=c('0'='deepskyblue2',
'1'='darkorange2'),
labels=c('No Default','Default')) +
labs(title='Histogram of Train Predictions') +
xlab('Theta') + ylab('Density') +
theme(plot.title=element_text(face='bold'))
}
# function for plotting pr curve
pr_curve <- function(recall,precision){
ggplot() +
geom_line(aes(recall,precision),color='deepskyblue2') +
labs(title='Precision-Recall Curve') +
xlab('Recall') + ylab('Precision') +
theme(plot.title=element_text(face='bold'))
}
# function for finding cut off
find_cut_off <- function(precision, cutoff, recall){
rate <- as.data.frame(cbind(Cutoff=cutoff,Precision=precision,Recall=recall))
rate$distance <- sqrt((rate[,2]^2)+(rate[,3]^2))
index <- which.min(rate$distance)
return(rate$Cutoff[index])
}
# function for calculating the log of the posterior
log_posterior <- function(beta){
probs <- c()
for (i in 1:nrow(train_X)){
power <- beta[1]+(beta[2]*train_X[i,1])+(beta[3]*train_X[i,2])+
(beta[4]*train_X[i,3])+(beta[5]*train_X[i,4])+(beta[6]*train_X[i,5])+
(beta[7]*train_X[i,6])
probs <- c(probs,exp(power)/(1+exp(power)))
}
if (any(probs==0)|any(probs==1)){
return(-Inf)
} else{
total <- 0
for (i in 1:length(probs)){
total <- total + log((probs[i]^train_y[i])*((1-probs[i])^(1-train_y[i])))
}
return(total)
}
}
# r logistic regression
logistic_regression <- function(beta_0,burnin,iter,sigma){
betas <- NULL
beta_t <- beta_0
for (i in 1:iter){
set.seed(143+i)
beta_p <- mvtnorm::rmvnorm(1,mean=beta_t,sigma=sigma)
log_r <- min(0,log_posterior(beta_p)-log_posterior(beta_t))
u <- runif(1,min=0,max=1)
if(log(u)<log_r){
beta_t <- beta_p
}
betas <- rbind(betas,beta_t)
}
if (burnin!=0)
betas <- betas %>% tail(-1*burnin)
return(betas)
}
# train logistic regression model
beta_0 <- rep(0,7)
mh_logistic <- logistic_regression(beta_0,750,2000,0.005*diag(7))
# output effective sample size for each beta
for (i in 1:7){
cat('Effective size for beta',i,' :',effectiveSize(mh_logistic[,i]),'\n')
}
# create trace plots
par(mfcol=c(3,3))
for (i in 1:7){
coda::traceplot(as.mcmc(mh_logistic[,i]),col='deepskyblue2',
main=paste('Traceplot for Beta',i))
}
# create density plots
par(mfcol=c(3,3))
for (i in 1:7){
densplot(as.mcmc(mh_logistic[,i]),col='deepskyblue2',
main=paste('Density Plot for Beta',i))
}
# output betas for logistic regression model
for (i in 1:7){
cat('beta',i,' :',mean(mh_logistic[,i]),'\n')
}
# create betas
mh_betas <- c()
for (i in 1:7) {
mh_betas <- c(mh_betas,mean(mh_logistic[,i]))
}
# create data frame
prob <- logistic_predictions(train_X,mh_betas)
mh_pred_train <- data.frame(prob)
mh_pred_train$observe <- as.factor(train_y)
# plot predictions
pred_plot(mh_pred_train$prob,mh_pred_train$observe)
# obtain precision and recall
mh_predict <- prediction(mh_pred_train$prob,mh_pred_train$observe)
mh_performance <- performance(mh_predict,'prec','rec')
# plot area under pr curve
pr_curve(performance(mh_predict, 'rec')@y.values[[1]],
performance(mh_predict, 'prec')@y.values[[1]])
# find cut off
mh_best <- find_cut_off(performance(mh_predict, 'prec')@y.values[[1]],
performance(mh_predict, 'prec')@x.values[[1]],
performance(mh_predict, 'rec')@y.values[[1]])
# output the result
cat('Cutoff: ',mh_best)
# use threshold to classify observations
mh_pred_train$pred <- ifelse(mh_pred_train$prob>=mh_best,'1','0')
# create confusion matrix
kable(table(pred=mh_pred_train$pred,true=mh_pred_train$observe),
col.names=c('No Default','Default'))%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
# create data frame
prob <- logistic_predictions(test_logistic_X,mh_betas)
mh_pred_test <- data.frame(prob)
mh_pred_test$observe <- as.factor(test_logistic_y)
# make predictions
mh_pred_test$pred <- ifelse(mh_pred_test$prob>=mh_best,'1','0')
# create confusion matrix
kable(table(pred=mh_pred_test$pred,true=mh_pred_test$observe),
col.names=c('No Default','Default'))%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
# logistic regression stan model
logistic_model <- stan_model('logistic_regression.stan')
# fit posterior distribution
set.seed(136)
stan_fit <- rstan::sampling(
logistic_model, data=
list(N=nrow(train_X),y=train_y,sigma=10,K=6,X=train_X),refresh=0)
# samples from posterior distribution
logistic_model_samples <- rstan::extract(stan_fit)
# create trace plots
par(mfcol=c(3,2))
for (i in 1:6){
coda::traceplot(as.mcmc(logistic_model_samples$beta[,i]),col='deepskyblue2',
main=paste('Traceplot for Beta',i))
}
# create density plots
par(mfcol=c(3,2))
for (i in 1:6){
densplot(as.mcmc(logistic_model_samples$beta[,i]),col='deepskyblue2',
main=paste('Density Plot for Beta',i))
}
# output betas for logistic regression model
cat('alpha:',mean(logistic_model_samples$alpha),'\n')
for (i in 1:6) {
cat('beta',i,': ',mean(logistic_model_samples$beta[,i]),'\n')
}
hmc_betas <- c(mean(logistic_model_samples$alpha))
for (i in 1:6){
hmc_betas <- c(hmc_betas,mean(logistic_model_samples$beta[,i]))
}
# create data frame
prob <- logistic_predictions(train_X,hmc_betas)
hmc_pred_train <- data.frame(prob)
hmc_pred_train$observe <- as.factor(train_y)
# plot predictions
pred_plot(hmc_pred_train$prob,hmc_pred_train$observe)
# obtain precision and recall
hmc_predict <- prediction(hmc_pred_train$prob,hmc_pred_train$observe)
hmc_performance <- performance(hmc_predict,'prec','rec')
# plot area under pr curve
pr_curve(performance(hmc_predict, 'rec')@y.values[[1]],
performance(hmc_predict, 'prec')@y.values[[1]])
# find cut off
hmc_best <- find_cut_off(performance(hmc_predict, 'prec')@y.values[[1]],
performance(hmc_predict, 'prec')@x.values[[1]],
performance(hmc_predict, 'rec')@y.values[[1]])
# output the result
cat('Cutoff: ',hmc_best)
# use threshold to classify observations
hmc_pred_train$pred <- ifelse(hmc_pred_train$prob>=hmc_best,'1','0')
# create confusion matrix
kable(table(pred=hmc_pred_train$pred,true=hmc_pred_train$observe),
col.names=c('No Default','Default'))%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
# create data frame
prob <- logistic_predictions(test_logistic_X,hmc_betas)
hmc_pred_test <- data.frame(prob)
hmc_pred_test$observe <- as.factor(test_logistic_y)
# make predictions
hmc_pred_test$pred <- ifelse(hmc_pred_test$prob>=hmc_best,'1','0')
# create confusion matrix
kable(table(pred=hmc_pred_test$pred,true=hmc_pred_test$observe),
col.names=c('No Default','Default'))%>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=FALSE,position='center')
nrow(test)
(765+170)/1340
# output betas for logistic regression model
for (i in 1:7){
cat('beta',i,' :',mean(mh_logistic[,i]),'\n')
}
head(train_X)
# create betas
mh_betas <- c()
for (i in 1:7) {
mh_betas <- c(mh_betas,mean(mh_logistic[,i]))
}
# create data frame
prob <- logistic_predictions(train_X,mh_betas)
mh_pred_train <- data.frame(prob)
mh_pred_train$observe <- as.factor(train_y)
# plot predictions
pred_plot(mh_pred_train$prob,mh_pred_train$observe)
nrow(test_y)
nrow(test_logistic_X)
(213+181)/1314
(749+171)/1314
